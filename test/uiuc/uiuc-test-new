#!/usr/bin/env python

"""a simple test of the interpolator
"""
__author__ = "Reed Essick (reed.essick@gmail.com), Ziyuan Zhang (ziyuan.z@wustl.edu)"

#-------------------------------------------------

import time
import sys, os
# sys.path.append('/Users/ziyuan/OneDrive - Washington University in St. \
# Louis/NeutronStar/Gaussian_Process_Interpolator/Reed_class_def/mgpi')
import pickle
import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_percentage_error
from scipy import interpolate
from scipy.optimize import minimize
import matplotlib
matplotlib.use("Agg")
from matplotlib import pyplot as plt

from argparse import ArgumentParser

### non-standard libraries
import mgpi
# import mgpi_new as mgpi


#--------------------------------------------------------------------------------------------------

DEFAULT_DOWNSAMPLE = 1
DEFAULT_MAX_TEMPERATURE = 400.0

chain_length = int(10000*1)
num_of_sample = 5


#-------------------------------------------------

parser = ArgumentParser()

#path = '../etc/equation_of_state.csv.gz'
parser.add_argument('eos_path', type=str)

parser.add_argument('--max-temperature', default=DEFAULT_MAX_TEMPERATURE, type=float,
    help='only consider data below this temperature. Should be specified in MeV. \
DEFAULT=%.3f MeV' % DEFAULT_MAX_TEMPERATURE)

parser.add_argument('--downsample', default=DEFAULT_DOWNSAMPLE, type=int,
    help='downsample the input data to keep only 1 sample out of this many. \
DEFAULT=%d' % DEFAULT_DOWNSAMPLE )

parser.add_argument('-v', '--verbose', default=False, action='store_true')
parser.add_argument('--time-execution', default=False, action='store_true')

parser.add_argument('-t', '--tag', default='', type=str)
parser.add_argument('--figtype', default=[], type=str, action='append')
parser.add_argument('--dpi', default=200, type=float)

args = parser.parse_args()

args.verbose |= args.time_execution

if args.tag:
    args.tag = "_"+args.tag

if not args.figtype:
    args.figtype.append('png')

#-------------------------------------------------

# let's build an interpolator over the speed of sound using half the points available
# the speed of sound seems to have the most interesting behavior, or at least features
# that might be the hardest for the GP to emulate accruately

if args.verbose:
    print('loading data from : '+args.eos_path)
    if args.time_execution:
        t0 = time.time()

# load sample data
data = np.genfromtxt(args.eos_path, delimiter=',', names=True)

# grab the relevant data
baryon_chemical_potential = data['muB_MeV']
temperature = data['T_MeV']
speed_of_sound = data['speed_of_sound']

### reshape
num = len(np.unique(baryon_chemical_potential))
shape = (len(data)//num, num)

if args.verbose:
    print('    found data with shape :', shape)
    if args.time_execution:
        print('    time : %.6f sec' % (time.time()-t0))

baryon_chemical_potential = baryon_chemical_potential.reshape(shape)
temperature = temperature.reshape(shape)
speed_of_sound = speed_of_sound.reshape(shape)

### throw away the boring part of the EoS

keep = temperature[:,0] <= args.max_temperature

baryon_chemical_potential = baryon_chemical_potential[keep,:]
temperature = temperature[keep,:]
speed_of_sound = speed_of_sound[keep,:]

if args.verbose:
    print('    downselected data to temperatures below %.3f MeV; retained shape : %s' % \
        (args.max_temperature, np.shape(speed_of_sound)))

### now decimate the data so that the covariance matrices we build fit in memory

baryon_chemical_potential = baryon_chemical_potential[::args.downsample, ::args.downsample]
temperature = temperature[::args.downsample, ::args.downsample]
speed_of_sound = speed_of_sound[::args.downsample, ::args.downsample]

if args.verbose:
    print('    further downselected data to shape :', np.shape(speed_of_sound))

#------------------------

# let's then divide this into training and testing sets
# we divid the grid in the following way: (o --> training set, x --> test set)
#     o o o o o
#     o x o x o
#     o o o o o
#     o x o x o
#     o o o o o

if args.verbose:
    print('formatting training and test sets')
    if args.time_execution:
        t0 = time.time()

source_x = []
source_f = []
target_x = []
target_f = []

for ind in range(len(speed_of_sound)):
    offset = ind % 2 ### figure out whether this is an odd or an even row

    if offset: # an odd row, so only take every other sample for training set
        # grab the training data
        source_x.append(np.transpose([baryon_chemical_potential[ind, ::2], temperature[ind, ::2]]))
        source_f.append(speed_of_sound[ind, ::2])

        # grab the test data
        target_x.append(np.transpose([baryon_chemical_potential[ind, 1::2], temperature[ind, 1::2]]))
        target_f.append(speed_of_sound[ind,1::2])

    else: # an even row, so grab everything
        # grab the training data
        source_x.append(np.transpose([baryon_chemical_potential[ind,:], temperature[ind,:]]))
        source_f.append(speed_of_sound[ind,:])

source_x = np.concatenate(tuple(source_x))
source_f = np.concatenate(tuple(source_f))

target_x = np.concatenate(tuple(target_x))
target_f = np.concatenate(tuple(target_f))

if args.verbose:
    print('    selected:\n        %d training points\n        %d test points'%(len(source_x), len(target_x)))
    if args.time_execution:
        print('    time : %.6f sec' % (time.time()-t0))


###--------------------------------------------------------------------------------------------------
### simple estimate of hyperparameters
###--------------------------------------------------------------------------------------------------
print("---------------------------------------------------------------------------------")
print("Building GP from a simple estimation of hyperparameters")

if args.verbose:
    print('estimating appropriate hyperparameters and constructing kernel')
    if args.time_execution:
        t0 = time.time()

# now, let's guess at appropriate hyperparameters
ave = np.mean(source_f)       ### mean of the process

### FIXME! pick these based on maximizing the marginal likelihood for (source_f, source_x)

sigma = np.std(source_f) * 1.5  ### variance based on spread of the function

lengths = [                   ### based on the average grid spacing
    np.mean(np.diff(baryon_chemical_potential[0,:])) * 2.0,
    np.mean(np.diff(temperature[:,0])) * 2.0,
]

# construct a kernel
kernel = mgpi.CombinedKernel(
    mgpi.SquaredExponentialKernel(sigma, *lengths),
    mgpi.WhiteNoiseKernel(sigma/1000), # a small white noise component for numerical stability
)

# construct a general interpolator
interpolator = mgpi.Interpolator(kernel)

if args.time_execution:
    print('    time : %.6f sec' % (time.time()-t0))

#------------------------

if args.verbose:
    print('computing interpolation estimate')

# now let's compute estimates of the speed of sound at the test points
mean, cov = interpolator.condition(
    target_x,
    source_x,
    source_f - ave,
    verbose=args.verbose,
    Verbose=args.time_execution,
)
mean += ave ### add back in the mean that I substracted

# print('Error (crude estimation of parameters): ', (mean-target_f)/np.diag(cov)**0.5)
print("---------------------------------GP simplely estimated hyperparameters---------------------------------------------")
print(f'Simple estimation of parameters: {[sigma, *lengths]}')
print(f"mape of GP simple estimation:  {mean_absolute_percentage_error(target_f, mean)}")
print('Error (simple estimation of parameters): ', np.mean((mean-target_f)/np.diag(cov)**0.5))

# print('end of test')
# exit()

###------------------------------------------------------------------------------------------------
### max likelihood using scipy.minimize(-loglikelihood)
###------------------------------------------------------------------------------------------------
print("---------------------------------GP scipy max likelihood-----------------------------------------------------------")
print("Start calculating hyperparameters that maximizes the log likelihood")

init_para = [sigma, *lengths]
optimize_kernel = mgpi.OptimizeKernel(kernel, source_x, source_f)

optimal_params = optimize_kernel.optimize_kernel(init_para)

print("Optimal hyperparameters:", optimal_params)

sigma_opt, *lengths_opt = optimal_params

# construct a kernel
kernel_optimized = mgpi.CombinedKernel(
    mgpi.SquaredExponentialKernel(sigma_opt, *lengths_opt),
    mgpi.WhiteNoiseKernel(sigma_opt/1000), # a small white noise component for numerical stability
)

optimized_para = mgpi.OptimizeKernel(kernel_optimized, source_x, source_f)

# now let's compute estimates of the speed of sound at the test points
mean_optimized, cov_optimized = optimized_para.condition(
    target_x,
    source_x,
    source_f - ave,
    verbose=False, #args.verbose,
    Verbose=False, #args.time_execution
)
mean_optimized += ave ### add back in the mean that I substracted

print(f"mape of GP max LLH:  {mean_absolute_percentage_error(target_f, mean_optimized)}")
print('Error (estimation max LLH): ', np.mean((mean_optimized-target_f)/np.diag(cov_optimized)**0.5))

# print('end of test')
# exit()


###--------------------------------------------------------------------------------------------------
###   sample parameters from likelihood function using MCMC
###--------------------------------------------------------------------------------------------------

# ------------------------------ new ------------------------------------
print("---------------------------------GP MCMC--------------------------------------------------------------------------")
print("Start sampling using MCMC")
# sample kernel parameters
sample_para = mgpi.SampleKernel(kernel, source_x, source_f)

# chain_length = int(10000*1)

autocorr_time_file_path = 'autocorrelation_time_uiuceos_downsample' + str(args.downsample) + '_chainlength' + str(chain_length) + '.log'
para_sample_file_path = 'parameter_samples_thinned_downsample' + str(args.downsample) + '_chainlength' + str(chain_length) + '.csv'

print(f"downsample {args.downsample}")
if not os.path.exists(para_sample_file_path) or not os.path.exists(autocorr_time_file_path):
    print('MC chain file does not exist. Run MCMC...')
    # how to thin the chain (TODO)
    init_para = [sigma, *lengths]
    print(f'initial guess for hyperparameters: {init_para}')

    # sampler is an emcee.EnsembleSampler() obj returned from sample_kernel
    start = time.time()
    sampler = sample_para.sample_kernel(init_para, burn_in=100, nsteps=chain_length, nwalkers=None)
    end = time.time()
    computation_time_autocorr = end - start
    print('    time : %.6f sec' % (computation_time_autocorr))
    print('sampling done')
    samples = sampler.get_chain(flat=True)
    # print(samples)
    # print(samples.shape)
    autocorr_time = np.mean(sampler.get_autocorr_time())
    print('autocorrelation estimation done')

    samples_thin = sampler.get_chain(flat=True, thin = int(autocorr_time))
    # print(samples_thin)
    # print(samples_thin.shape)
    print('sample thinning done')

    col_name = ['para'+str(i) for i in range(1, len(init_para)+1)]
    samples_df = pd.DataFrame(samples_thin, columns=col_name)
    samples_df.to_csv(para_sample_file_path, index=False)

    # Saving autocorrelation time to a text file
    with open(autocorr_time_file_path, 'w') as file:
        file.write(f"mean autocorrelation time: {autocorr_time}; computation time: {computation_time_autocorr} sec")
    print(f"autocorrelation time: {autocorr_time}")

    print("Data saved. (samples and antucorrelation time)")

elif os.path.exists(para_sample_file_path) and os.path.exists(autocorr_time_file_path):
    print(f'Reading MC chain from existing file <{para_sample_file_path}>')
    samples_thin = pd.read_csv(para_sample_file_path).values

    with open(autocorr_time_file_path, 'r') as file:
        autocorr_time = [line.strip() for line in file]
    print(f'autocorrelation time: {autocorr_time}')

print("-------------------------------------")

###------------------------------------------------------------------
### random sample from the chain
###------------------------------------------------------------------

sum_of_mean_sample = np.zeros(len(target_f))
sum_of_covar_sample = np.zeros((len(target_f),len(target_f)))

# num_of_sample = 15
for i in range(num_of_sample):
    random_index = np.random.choice(samples_thin.shape[0])
    random_row = samples_thin[random_index]
    print(f"sampling from the chain... sample #{i+1}, selected row: {random_row}")

    sigma_sample, *lengths_sample = random_row

    # construct a kernel
    kernel_sample = mgpi.CombinedKernel(
        mgpi.SquaredExponentialKernel(sigma_sample, *lengths_sample),
        mgpi.WhiteNoiseKernel(sigma_sample/1000), # a small white noise component for numerical stability
    )
    # print("kernel constructed")

    # now let's compute estimates of the speed of sound at the test points
    sample_para = mgpi.SampleKernel(kernel_sample, source_x, source_f)

    mean_sample, cov_sample = sample_para.condition(
        target_x,
        source_x,
        source_f - ave,
        verbose=False, #args.verbose,
        Verbose=False, #args.time_execution
    )
    mean_sample += ave ### add back in the mean that I substracted

    sum_of_mean_sample += mean_sample
    sum_of_covar_sample += cov_sample

avg_of_mean_sample = sum_of_mean_sample/float(num_of_sample)
avg_of_covar_sample = sum_of_covar_sample/float(num_of_sample)

print(f"mape of GP MCMC sampling:  {mean_absolute_percentage_error(target_f, avg_of_mean_sample)}")
# print('Error (MCMC sampling + avg)', np.mean((avg_of_mean_sample-target_f)/np.diag(avg_of_covar_sample)**0.5))



###------------------------------------------------------------------------------------------------
### linear interpolator
###------------------------------------------------------------------------------------------------
print("----------------------------------scipy LinearNDInterpolator--------------------------------------------------------")
lin_interp = interpolate.LinearNDInterpolator(source_x, source_f, rescale=True)
y_pred_lin = lin_interp(target_x)

print(f"mape of linear interpolator:  {mean_absolute_percentage_error(target_f, y_pred_lin)}")


#------------------------------------------------------------------------------------------------
# result analysis
#------------------------------------------------------------------------------------------------
print("----------------------------------Result Summary--------------------------------------------------------------------")
res = pd.DataFrame(target_x, columns=["nB","YQ"])
res['csq'] = target_f
res['csq_GP_simple'] = mean
res['csq_lin'] = y_pred_lin
res['csq_GP_MCMC_maxLLH'] = mean_optimized
res['csq_GP_MCMC_sample'] = avg_of_mean_sample
res = res.dropna()

print("Summary (MAPE):")
print(f"mape of linear interpolator:   {mean_absolute_percentage_error(res['csq'],res['csq_lin'])}")
print(f"mape of GP simple estimate:    {mean_absolute_percentage_error(res['csq'],res['csq_GP_simple'])}")
print(f"mape of GP max LLH esimate:    {mean_absolute_percentage_error(res['csq'],res['csq_GP_MCMC_maxLLH'])}")
print(f"mape of GP MCMC {num_of_sample} sample avg:  {mean_absolute_percentage_error(res['csq'],res['csq_GP_MCMC_sample'])}")

print("Summary (Error/variance):")
print('Error (simple estimation):  ', np.mean((mean-target_f)/np.diag(cov)**0.5))
print('Error (estimation max LLH): ', np.mean((mean_optimized-target_f)/np.diag(cov_optimized)**0.5))
# print('Error (MCMC sampling + avg):', np.mean((avg_of_mean_sample-target_f)/np.diag(avg_of_covar_sample)**0.5))




exit()

#------------------------

# make some plots of the predicted mean and covariance
# in particular, compare this to the true function : target_f = f(target_x)

#------------------------

if args.verbose:
    print('histogram the errors in the interpolator')
    if args.time_execution:
        t0 = time.time()

fig = plt.figure(figsize=(8,4))

nbins = int(len(target_f)**0.5)

# absolute error
ax = plt.subplot(1,2,1)
ax.hist(mean-target_f, bins=nbins, histtype='step', log=True)
ax.set_xlabel('$E[f(x)] - f(x)$')
ax.tick_params(
    left=True,
    right=True,
    top=True,
    bottom=True,
    which='both',
    direction='in',
)

# scaled error
ax = plt.subplot(1,2,2)
ax.hist((mean-target_f)/np.diag(cov)**0.5, bins=nbins, histtype='step', log=True)
ax.set_xlabel('$(E[f(x)] - f(x)]/\sigma_{f(x)}$')
ax.tick_params(
    left=True,
    right=True,
    top=True,
    bottom=True,
    which='both',
    direction='in',
)

# save
plt.subplots_adjust(
    left=0.10,
    bottom=0.15,
    top=0.98,
    right=0.95,
    wspace=0.15,
)

tmp = 'error-histograms%s.' % args.tag
for figtype in args.figtype:
    path = tmp + figtype
    if args.verbose:
        print('    saving : '+path)
    fig.savefig(path, dpi=args.dpi)
plt.close(fig)

if args.time_execution:
    print('    time : %.6f sec' % (time.time()-t0))

#-----------

# plot the distribution of errors across the function's domain

t0 = time.time()

# first, reshape the target data into something I can use
target_bcp, target_tmp = np.transpose(target_x)

num = len(np.unique(target_bcp))
shape = (len(target_bcp)//num, num)

target_bcp = target_bcp.reshape(shape)[0,:]
target_tmp = target_tmp.reshape(shape)[:,0]
target_f = target_f.reshape(shape)
mean = mean.reshape(shape)
stdv = (np.diag(cov)**0.5).reshape(shape)

vmin = np.min([mean, target_f])
vmax = np.max([mean, target_f])

# now make some plots

fig = plt.figure(figsize=(8,6))

# true function
ax = plt.subplot(2,3,1)
mappable = ax.imshow(
    target_f,
    extent=[np.min(target_bcp), np.max(target_bcp), np.min(target_tmp), np.max(target_tmp)],
    vmin=vmin,
    vmax=vmax,
    aspect='auto',
    origin='lower',
)
plt.setp(ax.get_xticklabels(), visible=False)
ax.set_ylabel('$T\,[\mathrm{MeV}]$')
ax.set_title('$f(x)$')
ax.tick_params(
    left=True,
    right=True,
    top=True,
    bottom=True,
    which='both',
    direction='in',
)

cb = plt.colorbar(mappable)

# mean
ax = plt.subplot(2,3,2)
mappable = ax.imshow(
    mean,
    extent=[np.min(target_bcp), np.max(target_bcp), np.min(target_tmp), np.max(target_tmp)],
    vmin=vmin,
    vmax=vmax,
    aspect='auto',
    origin='lower',
)
plt.setp(ax.get_xticklabels(), visible=False)
plt.setp(ax.get_yticklabels(), visible=False)
ax.set_title('$E[f(x)]$')
ax.tick_params(
    left=True,
    right=True,
    top=True,
    bottom=True,
    which='both',
    direction='in',
)

cb = plt.colorbar(mappable)

# uncertainty
ax = plt.subplot(2,3,3)
mappable = ax.imshow(
    stdv,
    extent=[np.min(target_bcp), np.max(target_bcp), np.min(target_tmp), np.max(target_tmp)],
    aspect='auto',
    origin='lower',
)
plt.setp(ax.get_xticklabels(), visible=False)
plt.setp(ax.get_yticklabels(), visible=False)
ax.set_title('$\sigma_{f(x)}$')
ax.tick_params(
    left=True,
    right=True,
    top=True,
    bottom=True,
    which='both',
    direction='in',
)

cb = plt.colorbar(mappable)

# absolute error
ax = plt.subplot(2,3,4)
mappable = ax.imshow(
    mean - target_f,
    extent=[np.min(target_bcp), np.max(target_bcp), np.min(target_tmp), np.max(target_tmp)],
    aspect='auto',
    origin='lower',
)
ax.set_xlabel('$\mu_B\,[\mathrm{MeV}]$')
ax.set_ylabel('$T\,[\mathrm{MeV}]$')
ax.set_title('$E[f(x)] - f(x)$')
ax.tick_params(
    left=True,
    right=True,
    top=True,
    bottom=True,
    which='both',
    direction='in',
)

cb = plt.colorbar(mappable)

# relative error
ax = plt.subplot(2,3,5)
mappable = ax.imshow(
    np.log10(np.abs(mean - target_f) / target_f),
    extent=[np.min(target_bcp), np.max(target_bcp), np.min(target_tmp), np.max(target_tmp)],
    aspect='auto',
    origin='lower',
)
ax.set_xlabel('$\mu_B\,[\mathrm{MeV}]$')
plt.setp(ax.get_yticklabels(), visible=False)
ax.set_title('$\log_{10}\left[|E[f(x)] - f(x)|/f(x)\\right]$')
ax.tick_params(
    left=True,
    right=True,
    top=True,
    bottom=True,
    which='both',
    direction='in',
)

cb = plt.colorbar(mappable)

# scaled error
ax = plt.subplot(2,3,6)
mappable = ax.imshow(
    (mean - target_f) / stdv,
    extent=[np.min(target_bcp), np.max(target_bcp), np.min(target_tmp), np.max(target_tmp)],
    aspect='auto',
    origin='lower',
)
ax.set_xlabel('$\mu_B\,[\mathrm{MeV}]$')
plt.setp(ax.get_yticklabels(), visible=False)
ax.set_title('$(E[f(x)] - f(x))/\sigma_{f(x)}$')
ax.tick_params(
    left=True,
    right=True,
    top=True,
    bottom=True,
    which='both',
    direction='in',
)

cb = plt.colorbar(mappable)

# save
plt.subplots_adjust(
    left=0.10,
    bottom=0.10,
    top=0.90,
    right=0.95,
    hspace=0.15,
    wspace=0.20,
)

tmp = 'error-surface%s.' % args.tag
for figtype in args.figtype:
    path = tmp + figtype
    if args.verbose:
        print('    saving : '+path)
    fig.savefig(path, dpi=args.dpi)
plt.close(fig)

if args.time_execution:
    print('    time : %.6f sec' % (time.time()-t0))
