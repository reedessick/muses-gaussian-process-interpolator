#!/usr/bin/env python

"""a simple test of the interpolator
"""
__author__ = "Reed Essick (reed.essick@gmail.com), Ziyuan Zhang (ziyuan.z@wustl.edu)"

#-------------------------------------------------

import time
import sys, os

import pickle
import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_percentage_error
from scipy import interpolate
from scipy.optimize import minimize
import matplotlib
matplotlib.use("Agg")
from matplotlib import pyplot as plt

from argparse import ArgumentParser

### non-standard libraries
# import mgpi
import mgpi

# print(sys.path)
# exit()


#-------------------------------------------------

DEFAULT_DOWNSAMPLE = 1
DEFAULT_MAX_TEMPERATURE = 400.0

#-------------------------------------------------

parser = ArgumentParser()

#path = '../etc/equation_of_state.csv.gz'
parser.add_argument('eos_path', type=str)

parser.add_argument('--max-temperature', default=DEFAULT_MAX_TEMPERATURE, type=float,
    help='only consider data below this temperature. Should be specified in MeV. \
DEFAULT=%.3f MeV' % DEFAULT_MAX_TEMPERATURE)

parser.add_argument('--downsample', default=DEFAULT_DOWNSAMPLE, type=int,
    help='downsample the input data to keep only 1 sample out of this many. \
DEFAULT=%d' % DEFAULT_DOWNSAMPLE )

parser.add_argument('-v', '--verbose', default=False, action='store_true')
parser.add_argument('--time-execution', default=False, action='store_true')

parser.add_argument('-t', '--tag', default='', type=str)
parser.add_argument('--figtype', default=[], type=str, action='append')
parser.add_argument('--dpi', default=200, type=float)

args = parser.parse_args()

args.verbose |= args.time_execution

if args.tag:
    args.tag = "_"+args.tag

if not args.figtype:
    args.figtype.append('png')

#-------------------------------------------------

# let's build an interpolator over the speed of sound using half the points available
# the speed of sound seems to have the most interesting behavior, or at least features
# that might be the hardest for the GP to emulate accruately

if args.verbose:
    print('loading data from : '+args.eos_path)
    if args.time_execution:
        t0 = time.time()

# load sample data
data = np.genfromtxt(args.eos_path, delimiter=',', names=True)

# grab the relevant data
baryon_chemical_potential = data['muB_MeV']
temperature = data['T_MeV']
speed_of_sound = data['speed_of_sound']

### reshape
num = len(np.unique(baryon_chemical_potential))
shape = (len(data)//num, num)

if args.verbose:
    print('    found data with shape :', shape)
    if args.time_execution:
        print('    time : %.6f sec' % (time.time()-t0))

baryon_chemical_potential = baryon_chemical_potential.reshape(shape)
temperature = temperature.reshape(shape)
speed_of_sound = speed_of_sound.reshape(shape)

### throw away the boring part of the EoS

keep = temperature[:,0] <= args.max_temperature

baryon_chemical_potential = baryon_chemical_potential[keep,:]
temperature = temperature[keep,:]
speed_of_sound = speed_of_sound[keep,:]

if args.verbose:
    print('    downselected data to temperatures below %.3f MeV; retained shape : %s' % \
        (args.max_temperature, np.shape(speed_of_sound)))

### now decimate the data so that the covariance matrices we build fit in memory

baryon_chemical_potential = baryon_chemical_potential[::args.downsample, ::args.downsample]
temperature = temperature[::args.downsample, ::args.downsample]
speed_of_sound = speed_of_sound[::args.downsample, ::args.downsample]

if args.verbose:
    print('    further downselected data to shape :', np.shape(speed_of_sound))

#------------------------

# let's then divide this into training and testing sets
# we divid the grid in the following way: (o --> training set, x --> test set)
#     o o o o o
#     o x o x o
#     o o o o o
#     o x o x o
#     o o o o o

if args.verbose:
    print('formatting training and test sets')
    if args.time_execution:
        t0 = time.time()

source_x = []
source_f = []
target_x = []
target_f = []

for ind in range(len(speed_of_sound)):
    offset = ind % 2 ### figure out whether this is an odd or an even row

    if offset: # an odd row, so only take every other sample for training set
        # grab the training data
        source_x.append(np.transpose([baryon_chemical_potential[ind, ::2], temperature[ind, ::2]]))
        source_f.append(speed_of_sound[ind, ::2])

        # grab the test data
        target_x.append(np.transpose([baryon_chemical_potential[ind, 1::2], temperature[ind, 1::2]]))
        target_f.append(speed_of_sound[ind,1::2])

    else: # an even row, so grab everything
        # grab the training data
        source_x.append(np.transpose([baryon_chemical_potential[ind,:], temperature[ind,:]]))
        source_f.append(speed_of_sound[ind,:])

source_x = np.concatenate(tuple(source_x))
source_f = np.concatenate(tuple(source_f))

target_x = np.concatenate(tuple(target_x))
target_f = np.concatenate(tuple(target_f))

if args.verbose:
    print('    selected:\n        %d training points\n        %d test points'%(len(source_x), len(target_x)))
    if args.time_execution:
        print('    time : %.6f sec' % (time.time()-t0))

###---------------------------------------------------------------------------------
### simple estimate of hyperparameters
###---------------------------------------------------------------------------------
print("---------------------------------------------------------------------------------")
print("Building GP from simple estimation of hyperparameters")

if args.verbose:
    print('estimating appropriate hyperparameters and constructing kernel')
    if args.time_execution:
        t0 = time.time()

# now, let's guess at appropriate hyperparameters
ave = np.mean(source_f)       ### mean of the process

### FIXME! pick these based on maximizing the marginal likelihood for (source_f, source_x)

sigma = np.std(source_f) * 1.5  ### variance based on spread of the function

lengths = [                   ### based on the average grid spacing
    np.mean(np.diff(baryon_chemical_potential[0,:])) * 2.0,
    np.mean(np.diff(temperature[:,0])) * 2.0,
]

# sigma, *lengths = [1.02906526e-01, 5.01750638e+02, 3.58583439e+01]
# print(f"sigma={sigma}, lengths={lengths}")

# construct a kernel
kernel = mgpi.CombinedKernel(
    mgpi.SquaredExponentialKernel(sigma, *lengths),
    mgpi.WhiteNoiseKernel(sigma/1000), # a small white noise component for numerical stability
)

# construct a general interpolator
interpolator = mgpi.Interpolator(kernel)

if args.time_execution:
    print('    time : %.6f sec' % (time.time()-t0))

#------------------------

if args.verbose:
    print('computing interpolation estimate')

# now let's compute estimates of the speed of sound at the test points
mean, cov = interpolator.condition(
    target_x,
    source_x,
    source_f - ave,
    verbose=args.verbose,
    Verbose=args.time_execution,
)
mean += ave ### add back in the mean that I substracted

# print('Error (crude estimation of parameters): ', (mean-target_f)/np.diag(cov)**0.5)
print(f'Simple estimation of parameters: {[sigma, *lengths]}')
print(f"mape of GP simple estimation:  {mean_absolute_percentage_error(target_f, mean)}")
print('Error (simple estimation of parameters): ', np.mean((mean-target_f)/np.diag(cov)**0.5))

# exit()

###------------------------------------------------------------------------------------------------
### max likelihood scipy.minimize()
###------------------------------------------------------------------------------------------------
print("---------------------------------------------------------------------------------")
print("Start calculating hyperparameters that maximizes the log likelihood")

# init_para_list = [sigma, *lengths]
# bound_list = [(init_para[i]/1e2, init_para[i]*1e2) for i in range(len(init_para))]
# optimal_params = interpolator.optimize_kernel(source_x, source_f, init_para, bound_list)

init_params = {'sigma': sigma, **{f'length{i+1}': length for i, length in enumerate(lengths)}}
bound_list = {key: (value / 1e3, value * 1e3) for key, value in init_params.items()}

optimal_params = interpolator.optimize_kernel(source_x, source_f, init_params, bound_list)

print("Optimal hyperparameters:", optimal_params)

# sigma_opt, *lengths_opt = optimal_params
# sigma_opt = optimal_params['sigma']
# lengths_opt = [optimal_params[key] for key in sorted(optimal_params) if key.startswith('length')]


# construct a kernel
# kernel_optimized = mgpi.CombinedKernel(
#     mgpi.SquaredExponentialKernel(sigma_opt, *lengths_opt),
#     mgpi.WhiteNoiseKernel(sigma_opt/1000), # a small white noise component for numerical stability
# )

# now let's compute estimates of the speed of sound at the test points
# optimized_para = mgpi.SampleKernel(kernel_optimized, source_x, source_f)

mean_optimized, cov_optimized = interpolator.condition(
    target_x,
    source_x,
    source_f - ave,
    verbose=False, #args.verbose,
    Verbose=False, #args.time_execution
)
mean_optimized += ave ### add back in the mean that I substracted

print(f"mape of GP max LLH:  {mean_absolute_percentage_error(target_f, mean_optimized)}")
print('Error (estimation max LLH): ', np.mean((mean_optimized-target_f)/np.diag(cov_optimized)**0.5))

# exit()


###--------------------------------------------------------------------------------------------------
###   sample parameters from likelihood function using MCMC
###--------------------------------------------------------------------------------------------------

# ------------------------------ new ------------------------------------
print("---------------------------------------------------------------------------------")
print("Start sampling using MCMC")
# sample kernel parameters
# sample_para = mgpi.SampleKernel(kernel, source_x, source_f)

chain_length = int(15000*1)
# samples_file_path = 'parameter_samples_thinned.csv'
autocorr_time_file_path = 'autocorrelation_time_uiuceos_downsample' + str(args.downsample) + '_chainlength' + str(chain_length) + '.log'
para_sample_file_path = 'parameter_samples_downsample' + str(args.downsample) + '_chainlength' + str(chain_length) + '.csv'
# autocorr_time_file_path = 'autocorrelation_time_uiuceos_downsample.log'
print(f"downsample {args.downsample}")
if not os.path.exists(para_sample_file_path) or not os.path.exists(autocorr_time_file_path):
    print('MC chain file does not exist. Run MCMC...')
    init_para = [sigma, *lengths]
    init_params_dict = {'sigma': sigma, **{f'length{i+1}': length for i, length in enumerate(lengths)}}
    print(f'initial guess for hyperparameters: {init_params_dict}')

    # sampler is an emcee.EnsembleSampler() obj returned from sample_kernel
    start = time.time()
    sampler = interpolator.sample_kernel(source_x, source_f, init_params_dict, burn_in=100, nsteps=chain_length, nwalkers=None)
    end = time.time()
    computation_time_autocorr = end - start
    print('    time : %.6f sec' % (computation_time_autocorr))
    print('sampling finished')

    # col_name = ['para'+str(i) for i in range(1, len(init_para)+1)]
    samples_all = sampler.get_chain(flat=True)
    col_name = init_params_dict.keys()
    samples_df = pd.DataFrame(samples_all, columns=col_name)
    samples_df.to_csv(para_sample_file_path, index=False)

    # get autocorr_time
    autocorr_time = np.mean(sampler.get_autocorr_time())
    print(f'autocorrelation done. autocorrelation = {autocorr_time}')
    # pick every #autocorr_time steps
    samples_thin = sampler.get_chain(flat=True, thin = int(autocorr_time))
    print('samples_thin done (picking samples every #autocorr_time steps)')

    # Saving autocorrelation time to a text file
    with open(autocorr_time_file_path, 'w') as file:
        file.write(f"mean autocorrelation time:\n")
        file.write(f"{autocorr_time}")
        file.write(f"\n")
        file.write(f"computation time (sec): \n")
        file.write(f"{computation_time_autocorr}")

    print("Data saved. (samples and antocorrelation time)")

elif os.path.exists(para_sample_file_path) and os.path.exists(autocorr_time_file_path):
    with open(autocorr_time_file_path, 'r') as file:
        read_info = [line.strip() for line in file]
        autocorr_time = float(read_info[1])
    print(f'autocorrelation time (sec): {autocorr_time}')

    # pick every #autocorr_time steps
    print(f'Reading MC chain from existing file <{para_sample_file_path}>')
    samples_thin = pd.read_csv(para_sample_file_path).values[::int(autocorr_time)]

# exit()
###------------------------------------------------------------------
### random sample from the chain
###------------------------------------------------------------------

sum_of_mean_sample = np.zeros(len(target_f))
sum_of_covar_sample = np.zeros((len(target_f),len(target_f)))
# print(samples_thin.shape)

num_of_sample = 15
for i in range(num_of_sample):
    random_index = np.random.choice(samples_thin.shape[0])
    # random_row = samples_thin.iloc[random_index]
    random_row = samples_thin[random_index]
    # print(f"sampling from the chain... sample #{i+1}, selected sample: {random_row}")
    # print(f"Randomly selected row: {random_row}")

    sigma_sample, *lengths_sample = random_row #[1.02906526e-01, 5.01750638e+02, 3.58583439e+01]
    sample_params_dict = {'sigma': sigma_sample, **{f'length{i+1}': length for i, length in enumerate(lengths_sample)}}
    # print(f"sample_params_dict = {sample_params_dict}")

    # construct a kernel
    # kernel_sample = mgpi.CombinedKernel(
    #     mgpi.SquaredExponentialKernel(sigma_sample, *lengths_sample),
    #     mgpi.WhiteNoiseKernel(sigma_sample/1000), # a small white noise component for numerical stability
    # )
    # print("kernel constructed")

    # now let's compute estimates of the speed of sound at the test points
    # sample_para = mgpi.SampleKernel(kernel_sample, source_x, source_f)

    # print(f"before update, sqexp:      sigma={interpolator.kernel.kernels[0].sigma}, lengths={interpolator.kernel.kernels[0].lengths}")
    # print(f"before update, whitenoise: sigma={interpolator.kernel.kernels[1].sigma}")

    interpolator.kernel.update(**sample_params_dict)
    # print(f"after update, sqexp:      sigma={interpolator.kernel.kernels[0].sigma}, lengths={interpolator.kernel.kernels[0].lengths}")
    # print(f"after update, whitenoise: sigma={interpolator.kernel.kernels[1].sigma}")

    mean_sample, cov_sample = interpolator.condition(
        target_x,
        source_x,
        source_f - ave,
        verbose=False, #args.verbose,
        Verbose=False, #args.time_execution
    )
    mean_sample += ave ### add back in the mean that I substracted

    # print(f"mean_sample: {mean_sample}")

    sum_of_mean_sample += mean_sample
    sum_of_covar_sample += cov_sample

avg_of_mean_sample = sum_of_mean_sample/float(num_of_sample)
avg_of_covar_sample = sum_of_covar_sample/float(num_of_sample)

print('Error estimate: average of the sample predictions', np.mean((avg_of_mean_sample-target_f)/np.diag(avg_of_covar_sample)**0.5))

# exit()

###------------------------------------------------------------------------------------------------
### linear interpolator
###------------------------------------------------------------------------------------------------
lin_interp = interpolate.LinearNDInterpolator(source_x, source_f, rescale=True)
y_pred_lin = lin_interp(target_x)


#------------------------------------------------------------------------------------------------
# result analysis
#------------------------------------------------------------------------------------------------
print("---------------------------------------------------------------------------------")
res = pd.DataFrame(target_x, columns=["nB","YQ"])
res['csq'] = target_f
res['csq_GP_simple'] = mean
res['csq_lin'] = y_pred_lin
res['csq_GP_scipy_maxLLH'] = mean_optimized
res['csq_GP_MCMC_sample'] = avg_of_mean_sample
res = res.dropna()

print("Summary of the results:")
print(f"mape of linear interpolator:   {mean_absolute_percentage_error(res['csq'],res['csq_lin'])}")
print(f"mape of GP simple estimate:    {mean_absolute_percentage_error(res['csq'],res['csq_GP_simple'])}")
print(f"mape of GP max LLH esimate:    {mean_absolute_percentage_error(res['csq'],res['csq_GP_scipy_maxLLH'])}")
print(f"mape of GP MCMC {num_of_sample} sample avg: {mean_absolute_percentage_error(res['csq'],res['csq_GP_MCMC_sample'])}")




exit()

#------------------------

# make some plots of the predicted mean and covariance
# in particular, compare this to the true function : target_f = f(target_x)

#------------------------

if args.verbose:
    print('histogram the errors in the interpolator')
    if args.time_execution:
        t0 = time.time()

fig = plt.figure(figsize=(8,4))

nbins = int(len(target_f)**0.5)

# absolute error
ax = plt.subplot(1,2,1)
ax.hist(mean-target_f, bins=nbins, histtype='step', log=True)
ax.set_xlabel('$E[f(x)] - f(x)$')
ax.tick_params(
    left=True,
    right=True,
    top=True,
    bottom=True,
    which='both',
    direction='in',
)

# scaled error
ax = plt.subplot(1,2,2)
ax.hist((mean-target_f)/np.diag(cov)**0.5, bins=nbins, histtype='step', log=True)
ax.set_xlabel('$(E[f(x)] - f(x)]/\sigma_{f(x)}$')
ax.tick_params(
    left=True,
    right=True,
    top=True,
    bottom=True,
    which='both',
    direction='in',
)

# save
plt.subplots_adjust(
    left=0.10,
    bottom=0.15,
    top=0.98,
    right=0.95,
    wspace=0.15,
)

tmp = 'error-histograms%s.' % args.tag
for figtype in args.figtype:
    path = tmp + figtype
    if args.verbose:
        print('    saving : '+path)
    fig.savefig(path, dpi=args.dpi)
plt.close(fig)

if args.time_execution:
    print('    time : %.6f sec' % (time.time()-t0))

#-----------

# plot the distribution of errors across the function's domain

t0 = time.time()

# first, reshape the target data into something I can use
target_bcp, target_tmp = np.transpose(target_x)

num = len(np.unique(target_bcp))
shape = (len(target_bcp)//num, num)

target_bcp = target_bcp.reshape(shape)[0,:]
target_tmp = target_tmp.reshape(shape)[:,0]
target_f = target_f.reshape(shape)
mean = mean.reshape(shape)
stdv = (np.diag(cov)**0.5).reshape(shape)

vmin = np.min([mean, target_f])
vmax = np.max([mean, target_f])

# now make some plots

fig = plt.figure(figsize=(8,6))

# true function
ax = plt.subplot(2,3,1)
mappable = ax.imshow(
    target_f,
    extent=[np.min(target_bcp), np.max(target_bcp), np.min(target_tmp), np.max(target_tmp)],
    vmin=vmin,
    vmax=vmax,
    aspect='auto',
    origin='lower',
)
plt.setp(ax.get_xticklabels(), visible=False)
ax.set_ylabel('$T\,[\mathrm{MeV}]$')
ax.set_title('$f(x)$')
ax.tick_params(
    left=True,
    right=True,
    top=True,
    bottom=True,
    which='both',
    direction='in',
)

cb = plt.colorbar(mappable)

# mean
ax = plt.subplot(2,3,2)
mappable = ax.imshow(
    mean,
    extent=[np.min(target_bcp), np.max(target_bcp), np.min(target_tmp), np.max(target_tmp)],
    vmin=vmin,
    vmax=vmax,
    aspect='auto',
    origin='lower',
)
plt.setp(ax.get_xticklabels(), visible=False)
plt.setp(ax.get_yticklabels(), visible=False)
ax.set_title('$E[f(x)]$')
ax.tick_params(
    left=True,
    right=True,
    top=True,
    bottom=True,
    which='both',
    direction='in',
)

cb = plt.colorbar(mappable)

# uncertainty
ax = plt.subplot(2,3,3)
mappable = ax.imshow(
    stdv,
    extent=[np.min(target_bcp), np.max(target_bcp), np.min(target_tmp), np.max(target_tmp)],
    aspect='auto',
    origin='lower',
)
plt.setp(ax.get_xticklabels(), visible=False)
plt.setp(ax.get_yticklabels(), visible=False)
ax.set_title('$\sigma_{f(x)}$')
ax.tick_params(
    left=True,
    right=True,
    top=True,
    bottom=True,
    which='both',
    direction='in',
)

cb = plt.colorbar(mappable)

# absolute error
ax = plt.subplot(2,3,4)
mappable = ax.imshow(
    mean - target_f,
    extent=[np.min(target_bcp), np.max(target_bcp), np.min(target_tmp), np.max(target_tmp)],
    aspect='auto',
    origin='lower',
)
ax.set_xlabel('$\mu_B\,[\mathrm{MeV}]$')
ax.set_ylabel('$T\,[\mathrm{MeV}]$')
ax.set_title('$E[f(x)] - f(x)$')
ax.tick_params(
    left=True,
    right=True,
    top=True,
    bottom=True,
    which='both',
    direction='in',
)

cb = plt.colorbar(mappable)

# relative error
ax = plt.subplot(2,3,5)
mappable = ax.imshow(
    np.log10(np.abs(mean - target_f) / target_f),
    extent=[np.min(target_bcp), np.max(target_bcp), np.min(target_tmp), np.max(target_tmp)],
    aspect='auto',
    origin='lower',
)
ax.set_xlabel('$\mu_B\,[\mathrm{MeV}]$')
plt.setp(ax.get_yticklabels(), visible=False)
ax.set_title('$\log_{10}\left[|E[f(x)] - f(x)|/f(x)\\right]$')
ax.tick_params(
    left=True,
    right=True,
    top=True,
    bottom=True,
    which='both',
    direction='in',
)

cb = plt.colorbar(mappable)

# scaled error
ax = plt.subplot(2,3,6)
mappable = ax.imshow(
    (mean - target_f) / stdv,
    extent=[np.min(target_bcp), np.max(target_bcp), np.min(target_tmp), np.max(target_tmp)],
    aspect='auto',
    origin='lower',
)
ax.set_xlabel('$\mu_B\,[\mathrm{MeV}]$')
plt.setp(ax.get_yticklabels(), visible=False)
ax.set_title('$(E[f(x)] - f(x))/\sigma_{f(x)}$')
ax.tick_params(
    left=True,
    right=True,
    top=True,
    bottom=True,
    which='both',
    direction='in',
)

cb = plt.colorbar(mappable)

# save
plt.subplots_adjust(
    left=0.10,
    bottom=0.10,
    top=0.90,
    right=0.95,
    hspace=0.15,
    wspace=0.20,
)

tmp = 'error-surface%s.' % args.tag
for figtype in args.figtype:
    path = tmp + figtype
    if args.verbose:
        print('    saving : '+path)
    fig.savefig(path, dpi=args.dpi)
plt.close(fig)

if args.time_execution:
    print('    time : %.6f sec' % (time.time()-t0))
