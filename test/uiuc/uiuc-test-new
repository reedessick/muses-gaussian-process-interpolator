#!/usr/bin/env python3

"""a simple test of the interpolator
"""
__author__ = "Reed Essick (reed.essick@gmail.com), Ziyuan Zhang (ziyuan.z@wustl.edu)"

#-------------------------------------------------

import time
import sys
import os

import numpy as np
import h5py

from scipy import interpolate # used for benchmark linear interpolator

import matplotlib
matplotlib.use("Agg")
from matplotlib import pyplot as plt

from argparse import ArgumentParser

### non-standard libraries
import mgpi

#-------------------------------------------------

DEFAULT_DOWNSAMPLE = 1
DEFAULT_MAX_TEMPERATURE = 400.0

#-------------------------------------------------

parser = ArgumentParser()

parser.add_argument('eos_path', type=str)

parser.add_argument('--max-temperature', default=DEFAULT_MAX_TEMPERATURE, type=float,
    help='only consider data below this temperature. Should be specified in MeV. \
DEFAULT=%.3f MeV' % DEFAULT_MAX_TEMPERATURE)

parser.add_argument('--downsample', default=DEFAULT_DOWNSAMPLE, type=int,
    help='downsample the input data to keep only 1 sample out of this many. \
DEFAULT=%d' % DEFAULT_DOWNSAMPLE )

parser.add_argument('--num-burnin', default=mgpi.DEFAULT_NUM_BURNIN, type=int)
parser.add_argument('--num-samples', default=mgpi.DEFAULT_NUM_SAMPLES, type=int)
parser.add_argument('--num-samples-for-average', default=50, type=int,
    help='the number of samples used when computing the average prediction')

parser.add_argument('--seed', default=None, type=int)

parser.add_argument('-v', '--verbose', default=False, action='store_true')
parser.add_argument('--time-execution', default=False, action='store_true')

parser.add_argument('-t', '--tag', default='', type=str)

args = parser.parse_args()

args.verbose |= args.time_execution

if args.tag:
    args.tag = "_"+args.tag

args.num_samples_for_average = min(args.num_samples_for_average, args.num_samples) # can't average over more samples than we draw

if args.seed is not None:
    if args.verbose:
        print('setting numpy.random.seed=%d' % args.seed)
    np.random.seed(args.seed)

#-------------------------------------------------

# let's build an interpolator over the speed of sound using half the points available
# the speed of sound seems to have the most interesting behavior, or at least features
# that might be the hardest for the GP to emulate accruately

if args.verbose:
    print('loading data from : '+args.eos_path)
    if args.time_execution:
        t0 = time.time()

# load sample data
data = np.genfromtxt(args.eos_path, delimiter=',', names=True)

# grab the relevant data
baryon_chemical_potential = data['muB_MeV']
temperature = data['T_MeV']
speed_of_sound = data['speed_of_sound']

### reshape
num = len(np.unique(baryon_chemical_potential))
shape = (len(data)//num, num)

if args.verbose:
    print('    found data with shape :', shape)
    if args.time_execution:
        print('    time : %.6f sec' % (time.time()-t0))

baryon_chemical_potential = baryon_chemical_potential.reshape(shape)
temperature = temperature.reshape(shape)
speed_of_sound = speed_of_sound.reshape(shape)

### throw away the boring part of the EoS

keep = temperature[:,0] <= args.max_temperature

baryon_chemical_potential = baryon_chemical_potential[keep,:]
temperature = temperature[keep,:]
speed_of_sound = speed_of_sound[keep,:]

if args.verbose:
    print('    downselected data to temperatures below %.3f MeV; retained shape : %s' % \
        (args.max_temperature, np.shape(speed_of_sound)))

### now decimate the data so that the covariance matrices we build fit in memory

baryon_chemical_potential = baryon_chemical_potential[::args.downsample, ::args.downsample]
temperature = temperature[::args.downsample, ::args.downsample]
speed_of_sound = speed_of_sound[::args.downsample, ::args.downsample]

if args.verbose:
    print('    further downselected data to shape :', np.shape(speed_of_sound))

#------------------------

# let's then divide this into training and testing sets
# we divid the grid in the following way: (o --> training set, x --> test set)
#     o o o o o
#     o x o x o
#     o o o o o
#     o x o x o
#     o o o o o

if args.verbose:
    print('formatting training and test sets')
    if args.time_execution:
        t0 = time.time()

source_x = []
source_f = []
target_x = []
target_f = []

for ind in range(len(speed_of_sound)):
    offset = ind % 2 ### figure out whether this is an odd or an even row

    if offset: # an odd row, so only take every other sample for training set
        # grab the training data
        source_x.append(np.transpose([baryon_chemical_potential[ind, ::2], temperature[ind, ::2]]))
        source_f.append(speed_of_sound[ind, ::2])

        # grab the test data
        target_x.append(np.transpose([baryon_chemical_potential[ind, 1::2], temperature[ind, 1::2]]))
        target_f.append(speed_of_sound[ind,1::2])

    else: # an even row, so grab everything
        # grab the training data
        source_x.append(np.transpose([baryon_chemical_potential[ind,:], temperature[ind,:]]))
        source_f.append(speed_of_sound[ind,:])

source_x = np.concatenate(tuple(source_x))
source_f = np.concatenate(tuple(source_f))

target_x = np.concatenate(tuple(target_x))
target_f = np.concatenate(tuple(target_f))

if args.verbose:
    print('    selected:\n        %d training points\n        %d test points'%(len(source_x), len(target_x)))
    if args.time_execution:
        print('    time : %.6f sec' % (time.time()-t0))

#-------------------------------------------------
#
# linear interpolator
#
#-------------------------------------------------

print('\n>>> Building a simple linear interpolator')

lin_interp = interpolate.LinearNDInterpolator(source_x, source_f, rescale=True)
y_pred_lin = lin_interp(target_x)

print('>>> Mean Absolute Error : %.6e' % np.mean(np.abs(y_pred_lin-target_f)))
print('>>> Mean Absolute Relative Error : %.6e' % np.mean(np.abs(y_pred_lin-target_f)/target_f))

#-------------------------------------------------
#
# simple estimate of hyperparameters
#
#-------------------------------------------------

print("\n>>> Building GP from simple ad hoc estimate for hyperparameters")

if args.verbose:
    print('estimating appropriate hyperparameters and constructing kernel')
    if args.time_execution:
        t0 = time.time()

# now, let's guess at appropriate hyperparameters
ave = np.mean(source_f)       ### mean of the process

sigma = np.std(source_f) * 2.0  ### variance based on spread of the function

lengths = [                   ### based on the average grid spacing
    np.mean(np.diff(baryon_chemical_potential[0,:])) * 2.0,
    np.mean(np.diff(temperature[:,0])) * 2.0,
]

# construct a kernel
kernel = mgpi.CombinedKernel(
    mgpi.SquaredExponentialKernel(sigma, *lengths),
    mgpi.WhiteNoiseKernel(sigma/10000), # a small white noise component for numerical stability
)

# construct a general interpolator
interpolator = mgpi.Interpolator(kernel)

if args.time_execution:
    print('    time : %.6f sec' % (time.time()-t0))

if args.verbose:
    print('>>> guestimate hyperparameters:')
    print(kernel)
    print('>>> loglike : %.6e' % interpolator.loglikelihood(source_x, source_f))

#------------------------

if args.verbose:
    print('computing interpolation estimate')

# now let's compute estimates of the speed of sound at the test points
mean, cov = interpolator.condition(
    target_x,
    source_x,
    source_f - ave,
    Verbose=args.time_execution,
)
mean += ave ### add back in the mean that I substracted

if args.verbose:
    print('>>> guestimate hyperparameters:')
    print(kernel)
    print('>>> loglike : %.6e' % interpolator.loglikelihood(source_x, source_f))

print('>>> Mean Absolute Error : %.6e' % np.mean(np.abs(mean-target_f)))
print('>>> Mean Absolute Relative Error : %.6e' % np.mean(np.abs(mean-target_f)/target_f))

#-------------------------------------------------
#
# max likelihood scipy.minimize()
#
#-------------------------------------------------

print("\n>>> maximizing loglikelihood to select hyperparameters")

# define a basic prior
def logprior(params):
    bounds = [
        (0.001, 5.0),    # sigma_0 --> SquaredExponentialKernel
        (0.1, 40.0),   # length0_0 --> SquaredExponentialKernel
        (0.1, 40.0),   # length1_0 --> SquaredExponentialKernel
        (0.000, 1.0), # sigma_1 --> WhiteNoiseKernel
    ]
    for p, (m, M) in zip(params, bounds):
        if (p < m) or (M < p):
            return -np.infty
    else:
        return 0.0

# optimize subject to this prior
optimal_params = interpolator.optimize_kernel(
    source_x,
    source_f,
    logprior=logprior,
    Verbose=args.time_execution,
)

# now let's compute estimates of the speed of sound at the test points
# optimized_para = mgpi.SampleKernel(kernel_optimized, source_x, source_f)

mean_optimized, cov_optimized = interpolator.condition(
    target_x,
    source_x,
    source_f - ave,
    Verbose=args.time_execution
)
mean_optimized += ave ### add back in the mean that I substracted

if args.verbose:
    print('>>> optimized hyperparameters:')
    print(kernel)
    print('>>> loglike : %.6e' % interpolator.loglikelihood(source_x, source_f))

print('>>> Mean Absolute Error : %.6e' % np.mean(np.abs(mean_optimized-target_f)))
print('>>> Mean Absolute Relative Error : %.6e' % np.mean(np.abs(mean_optimized-target_f)/target_f))

#-------------------------------------------------
#
#   sample parameters from likelihood function using MCMC
#
#-------------------------------------------------

print("\n>>> sampling hyperparameters from distribution defined by loglikelihood")

# obtain samples
samples, logprob, sampler = interpolator.sample_kernel(
    source_x,
    source_f,
    logprior=logprior,
    num_burnin=args.num_burnin,
    num_samples=args.num_samples,
    num_walkers=None, # choose this automatically
    Verbose=args.time_execution,
)

# save samples to disk
outpath = os.path.basename(__file__) + '-samples.hdf'
if args.verbose:
    print('saving samples to: '+outpath)
with h5py.File(outpath, 'w') as obj:
    obj.create_dataset(data=kernel._params)
    obj.create_dataset(data=samples)

#------------------------

# average over the chain to marginalize over hyperparameters

if args.verbose:
    print('marginalizing over %d samples to compute average prediction' % args.num_samples_for_average)

mean_marginalized = 0.0

# generate a random subset of samples
samples = np.transpose([samples[:,:,ind].flatten() for ind in range(len(kernel._params))]+[logprob.flatten()])

order = np.arange(len(samples[0]))
np.random.shuffle(order)

for params in samples[:args.num_samples_for_average]:

    # update hyperparameters
    kernel.update(**dict(zip(kernel._params, params[:-1])))

    if args.verbose:
        print('>>> hyperparameters:')
        print(kernel)
        print('>>> loglike : %.6e' % params[-1])

    # predict
    _mean, _ = interpolator.condition(
        target_x,
        source_x,
        source_f - ave,
        Verbose=args.time_execution
    )
    _mean += ave ### add back in the mean that I substracted

    # add to running average
    mean_marginalized += _mean

# average over the number of samples taken
mean_marginalize /= args.num_samples_for_average

# report
print('>>> Mean Absolute Error : %.6e' % np.mean(np.abs(mean_marginalized-target_f)))
print('>>> Mean Absolute Relative Error : %.6e' % np.mean(np.abs(mean_marginalized-target_f)/target_f))
