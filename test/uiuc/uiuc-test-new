#!/usr/bin/env python3

"""a simple test of the interpolator
"""
__author__ = "Reed Essick (reed.essick@gmail.com), Ziyuan Zhang (ziyuan.z@wustl.edu)"

#-------------------------------------------------

import time
import sys
import os

import numpy as np
#import pandas as pd

#from sklearn.metrics import mean_absolute_percentage_error

from scipy import interpolate

import matplotlib
matplotlib.use("Agg")
from matplotlib import pyplot as plt

from argparse import ArgumentParser

### non-standard libraries
import mgpi

#-------------------------------------------------

DEFAULT_DOWNSAMPLE = 1
DEFAULT_MAX_TEMPERATURE = 400.0

#-------------------------------------------------

parser = ArgumentParser()

parser.add_argument('eos_path', type=str)

parser.add_argument('--max-temperature', default=DEFAULT_MAX_TEMPERATURE, type=float,
    help='only consider data below this temperature. Should be specified in MeV. \
DEFAULT=%.3f MeV' % DEFAULT_MAX_TEMPERATURE)

parser.add_argument('--downsample', default=DEFAULT_DOWNSAMPLE, type=int,
    help='downsample the input data to keep only 1 sample out of this many. \
DEFAULT=%d' % DEFAULT_DOWNSAMPLE )

parser.add_argument('-v', '--verbose', default=False, action='store_true')
parser.add_argument('--time-execution', default=False, action='store_true')

parser.add_argument('-t', '--tag', default='', type=str)
parser.add_argument('--figtype', default=[], type=str, action='append')
parser.add_argument('--dpi', default=200, type=float)

args = parser.parse_args()

args.verbose |= args.time_execution

if args.tag:
    args.tag = "_"+args.tag

if not args.figtype:
    args.figtype.append('png')

#-------------------------------------------------

# let's build an interpolator over the speed of sound using half the points available
# the speed of sound seems to have the most interesting behavior, or at least features
# that might be the hardest for the GP to emulate accruately

if args.verbose:
    print('loading data from : '+args.eos_path)
    if args.time_execution:
        t0 = time.time()

# load sample data
data = np.genfromtxt(args.eos_path, delimiter=',', names=True)

# grab the relevant data
baryon_chemical_potential = data['muB_MeV']
temperature = data['T_MeV']
speed_of_sound = data['speed_of_sound']

### reshape
num = len(np.unique(baryon_chemical_potential))
shape = (len(data)//num, num)

if args.verbose:
    print('    found data with shape :', shape)
    if args.time_execution:
        print('    time : %.6f sec' % (time.time()-t0))

baryon_chemical_potential = baryon_chemical_potential.reshape(shape)
temperature = temperature.reshape(shape)
speed_of_sound = speed_of_sound.reshape(shape)

### throw away the boring part of the EoS

keep = temperature[:,0] <= args.max_temperature

baryon_chemical_potential = baryon_chemical_potential[keep,:]
temperature = temperature[keep,:]
speed_of_sound = speed_of_sound[keep,:]

if args.verbose:
    print('    downselected data to temperatures below %.3f MeV; retained shape : %s' % \
        (args.max_temperature, np.shape(speed_of_sound)))

### now decimate the data so that the covariance matrices we build fit in memory

baryon_chemical_potential = baryon_chemical_potential[::args.downsample, ::args.downsample]
temperature = temperature[::args.downsample, ::args.downsample]
speed_of_sound = speed_of_sound[::args.downsample, ::args.downsample]

if args.verbose:
    print('    further downselected data to shape :', np.shape(speed_of_sound))

#------------------------

# let's then divide this into training and testing sets
# we divid the grid in the following way: (o --> training set, x --> test set)
#     o o o o o
#     o x o x o
#     o o o o o
#     o x o x o
#     o o o o o

if args.verbose:
    print('formatting training and test sets')
    if args.time_execution:
        t0 = time.time()

source_x = []
source_f = []
target_x = []
target_f = []

for ind in range(len(speed_of_sound)):
    offset = ind % 2 ### figure out whether this is an odd or an even row

    if offset: # an odd row, so only take every other sample for training set
        # grab the training data
        source_x.append(np.transpose([baryon_chemical_potential[ind, ::2], temperature[ind, ::2]]))
        source_f.append(speed_of_sound[ind, ::2])

        # grab the test data
        target_x.append(np.transpose([baryon_chemical_potential[ind, 1::2], temperature[ind, 1::2]]))
        target_f.append(speed_of_sound[ind,1::2])

    else: # an even row, so grab everything
        # grab the training data
        source_x.append(np.transpose([baryon_chemical_potential[ind,:], temperature[ind,:]]))
        source_f.append(speed_of_sound[ind,:])

source_x = np.concatenate(tuple(source_x))
source_f = np.concatenate(tuple(source_f))

target_x = np.concatenate(tuple(target_x))
target_f = np.concatenate(tuple(target_f))

if args.verbose:
    print('    selected:\n        %d training points\n        %d test points'%(len(source_x), len(target_x)))
    if args.time_execution:
        print('    time : %.6f sec' % (time.time()-t0))

#-------------------------------------------------
#
# linear interpolator
#
#-------------------------------------------------

print('>>> Building a simple linear interpolator')

lin_interp = interpolate.LinearNDInterpolator(source_x, source_f, rescale=True)
y_pred_lin = lin_interp(target_x)

print('>>> Mean Absolute Error : %.6e' % np.mean(np.abs(y_pred_lin-target_f)))
print('>>> Mean Absolute Relative Error : %.6e' % np.mean(np.abs(y_pred_lin-target_f)/target_f))

#-------------------------------------------------
#
# simple estimate of hyperparameters
#
#-------------------------------------------------

print("\n>>> Building GP from simple ad hoc estimate for hyperparameters")

if args.verbose:
    print('estimating appropriate hyperparameters and constructing kernel')
    if args.time_execution:
        t0 = time.time()

# now, let's guess at appropriate hyperparameters
ave = np.mean(source_f)       ### mean of the process

sigma = np.std(source_f) * 2.0  ### variance based on spread of the function

lengths = [                   ### based on the average grid spacing
    np.mean(np.diff(baryon_chemical_potential[0,:])) * 2.0,
    np.mean(np.diff(temperature[:,0])) * 2.0,
]

# construct a kernel
kernel = mgpi.CombinedKernel(
    mgpi.SquaredExponentialKernel(sigma, *lengths),
    mgpi.WhiteNoiseKernel(sigma/10000), # a small white noise component for numerical stability
)

# construct a general interpolator
interpolator = mgpi.Interpolator(kernel)

if args.time_execution:
    print('    time : %.6f sec' % (time.time()-t0))

if args.verbose:
    print('>>> guestimate hyperparameters:')
    print(kernel)
    print('>>> loglike : %.6e' % interpolator.loglikelihood(source_x, source_f))

#------------------------

if args.verbose:
    print('computing interpolation estimate')

# now let's compute estimates of the speed of sound at the test points
mean, cov = interpolator.condition(
    target_x,
    source_x,
    source_f - ave,
    Verbose=args.time_execution,
)
mean += ave ### add back in the mean that I substracted

if args.verbose:
    print('>>> guestimate hyperparameters:')
    print(kernel)
    print('>>> loglike : %.6e' % interpolator.loglikelihood(source_x, source_f))

print('>>> Mean Absolute Error : %.6e' % np.mean(np.abs(mean-target_f)))
print('>>> Mean Absolute Relative Error : %.6e' % np.mean(np.abs(mean-target_f)/target_f))

#-------------------------------------------------
#
# max likelihood scipy.minimize()
#
#-------------------------------------------------

print("\n>>> maximizing loglikelihood to select hyperparameters")

# define a basic prior
def logprior(params):
    bounds = [
        (0.001, 5.0),    # sigma_0 --> SquaredExponentialKernel
        (0.1, 40.0),   # length0_0 --> SquaredExponentialKernel
        (0.1, 40.0),   # length1_0 --> SquaredExponentialKernel
        (0.000, 1.0), # sigma_1 --> WhiteNoiseKernel
    ]
    for p, (m, M) in zip(params, bounds):
        if (p < m) or (M < p):
            return -np.infty
    else:
        return 0.0

# optimize subject to this prior
optimal_params = interpolator.optimize_kernel(
    source_x,
    source_f,
    logprior=logprior,
    Verbose=args.time_execution,
)

# now let's compute estimates of the speed of sound at the test points
# optimized_para = mgpi.SampleKernel(kernel_optimized, source_x, source_f)

mean_optimized, cov_optimized = interpolator.condition(
    target_x,
    source_x,
    source_f - ave,
    Verbose=args.time_execution
)
mean_optimized += ave ### add back in the mean that I substracted

if args.verbose:
    print('>>> optimized hyperparameters:')
    print(kernel)
    print('>>> loglike : %.6e' % interpolator.loglikelihood(source_x, source_f))

print('>>> Mean Absolute Error : %.6e' % np.mean(np.abs(mean_optimized-target_f)))
print('>>> Mean Absolute Relative Error : %.6e' % np.mean(np.abs(mean_optimized-target_f)/target_f))

#-------------------------------------------------
#
#   sample parameters from likelihood function using MCMC
#
#-------------------------------------------------

print("\n>>> sampling hyperparameters from distribution defined by loglikelihood")

raise NotImplementedError('Reed stopped here')

















chain_length = int(15000*1)

# samples_file_path = 'parameter_samples_thinned.csv'
autocorr_time_file_path = 'autocorrelation_time_uiuceos_downsample' + str(args.downsample) + '_chainlength' + str(chain_length) + '.log'
para_sample_file_path = 'parameter_samples_downsample' + str(args.downsample) + '_chainlength' + str(chain_length) + '.csv'

# autocorr_time_file_path = 'autocorrelation_time_uiuceos_downsample.log'

print(f"downsample {args.downsample}")

if not os.path.exists(para_sample_file_path) or not os.path.exists(autocorr_time_file_path):
    print('MC chain file does not exist. Run MCMC...')
    init_para = [sigma, *lengths]
    init_params_dict = {'sigma': sigma, **{f'length{i+1}': length for i, length in enumerate(lengths)}}
    print(f'initial guess for hyperparameters: {init_params_dict}')

    # sampler is an emcee.EnsembleSampler() obj returned from sample_kernel
    start = time.time()
    sampler = interpolator.sample_kernel(source_x, source_f, init_params_dict, burn_in=100, nsteps=chain_length, nwalkers=None)
    end = time.time()
    computation_time_autocorr = end - start
    print('    time : %.6f sec' % (computation_time_autocorr))
    print('sampling finished')

    # col_name = ['para'+str(i) for i in range(1, len(init_para)+1)]
    samples_all = sampler.get_chain(flat=True)
    col_name = init_params_dict.keys()
#    samples_df = pd.DataFrame(samples_all, columns=col_name)
#    samples_df.to_csv(para_sample_file_path, index=False)

    # get autocorr_time
    autocorr_time = np.mean(sampler.get_autocorr_time())
    print(f'autocorrelation done. autocorrelation = {autocorr_time}')
    # pick every #autocorr_time steps
    samples_thin = sampler.get_chain(flat=True, thin = int(autocorr_time))
    print('samples_thin done (picking samples every #autocorr_time steps)')

    # Saving autocorrelation time to a text file
    with open(autocorr_time_file_path, 'w') as file:
        file.write(f"mean autocorrelation time:\n")
        file.write(f"{autocorr_time}")
        file.write(f"\n")
        file.write(f"computation time (sec): \n")
        file.write(f"{computation_time_autocorr}")

    print("Data saved. (samples and antocorrelation time)")

elif os.path.exists(para_sample_file_path) and os.path.exists(autocorr_time_file_path):
    with open(autocorr_time_file_path, 'r') as file:
        read_info = [line.strip() for line in file]
        autocorr_time = float(read_info[1])
    print(f'autocorrelation time (sec): {autocorr_time}')

    # pick every #autocorr_time steps
    print(f'Reading MC chain from existing file <{para_sample_file_path}>')
#    samples_thin = pd.read_csv(para_sample_file_path).values[::int(autocorr_time)]

#------------------------

### consider a random sample from the chain

sum_of_mean_sample = np.zeros(len(target_f))
sum_of_covar_sample = np.zeros((len(target_f),len(target_f)))
# print(samples_thin.shape)

num_of_sample = 15
for i in range(num_of_sample):
    random_index = np.random.choice(samples_thin.shape[0])
    # random_row = samples_thin.iloc[random_index]
    random_row = samples_thin[random_index]
    # print(f"sampling from the chain... sample #{i+1}, selected sample: {random_row}")
    # print(f"Randomly selected row: {random_row}")

    sigma_sample, *lengths_sample = random_row #[1.02906526e-01, 5.01750638e+02, 3.58583439e+01]
    sample_params_dict = {'sigma': sigma_sample, **{f'length{i+1}': length for i, length in enumerate(lengths_sample)}}
    # print(f"sample_params_dict = {sample_params_dict}")

    # construct a kernel
    # kernel_sample = mgpi.CombinedKernel(
    #     mgpi.SquaredExponentialKernel(sigma_sample, *lengths_sample),
    #     mgpi.WhiteNoiseKernel(sigma_sample/1000), # a small white noise component for numerical stability
    # )
    # print("kernel constructed")

    # now let's compute estimates of the speed of sound at the test points
    # sample_para = mgpi.SampleKernel(kernel_sample, source_x, source_f)

    # print(f"before update, sqexp:      sigma={interpolator.kernel.kernels[0].sigma}, lengths={interpolator.kernel.kernels[0].lengths}")
    # print(f"before update, whitenoise: sigma={interpolator.kernel.kernels[1].sigma}")

    interpolator.kernel.update(**sample_params_dict)
    # print(f"after update, sqexp:      sigma={interpolator.kernel.kernels[0].sigma}, lengths={interpolator.kernel.kernels[0].lengths}")
    # print(f"after update, whitenoise: sigma={interpolator.kernel.kernels[1].sigma}")

    mean_sample, cov_sample = interpolator.condition(
        target_x,
        source_x,
        source_f - ave,
        verbose=args.verbose,
        Verbose=args.time_execution
    )
    mean_sample += ave ### add back in the mean that I substracted

    # print(f"mean_sample: {mean_sample}")

    sum_of_mean_sample += mean_sample
    sum_of_covar_sample += cov_sample

avg_of_mean_sample = sum_of_mean_sample/float(num_of_sample)
avg_of_covar_sample = sum_of_covar_sample/float(num_of_sample)

print('Error estimate: average of the sample predictions', np.mean((avg_of_mean_sample-target_f)/np.diag(avg_of_covar_sample)**0.5))
