#!/usr/bin/env python3

"""a simple test of the interpolator
"""
__author__ = "Reed Essick (reed.essick@gmail.com), Ziyuan Zhang (ziyuan.z@wustl.edu)"

#-------------------------------------------------

import time
import sys
import os

import numpy as np
import h5py

from scipy import interpolate # used for benchmark linear interpolator

import matplotlib
matplotlib.use("Agg")
from matplotlib import pyplot as plt

from argparse import ArgumentParser

### non-standard libraries
import mgpi

#-------------------------------------------------

DEFAULT_DOWNSAMPLE = 1
DEFAULT_MAX_TEMPERATURE = 400.0

#-------------------------------------------------

parser = ArgumentParser()

parser.add_argument('eos_path', type=str)

parser.add_argument('--max-temperature', default=DEFAULT_MAX_TEMPERATURE, type=float,
    help='only consider data below this temperature. Should be specified in MeV. \
DEFAULT=%.3f MeV' % DEFAULT_MAX_TEMPERATURE)

parser.add_argument('--downsample', default=DEFAULT_DOWNSAMPLE, type=int,
    help='downsample the input data to keep only 1 sample out of this many. \
DEFAULT=%d' % DEFAULT_DOWNSAMPLE )

parser.add_argument('--num-neighbors', default=10, type=int)
parser.add_argument('--order-by-index', default=0, type=int)

parser.add_argument('--num-trials', default=100, type=int)

parser.add_argument('-v', '--verbose', default=False, action='store_true')
parser.add_argument('--time-execution', default=False, action='store_true')

args = parser.parse_args()

args.verbose |= args.time_execution

#-------------------------------------------------

# let's build an interpolator over the speed of sound using half the points available
# the speed of sound seems to have the most interesting behavior, or at least features
# that might be the hardest for the GP to emulate accruately

if args.verbose:
    print('loading data from : '+args.eos_path)
    if args.time_execution:
        t0 = time.time()

# load sample data
data = np.genfromtxt(args.eos_path, delimiter=',', names=True)

# grab the relevant data
baryon_chemical_potential = data['muB_MeV']
temperature = data['T_MeV']
speed_of_sound = data['speed_of_sound']

### reshape
num = len(np.unique(baryon_chemical_potential))
shape = (len(data)//num, num)

if args.verbose:
    print('    found data with shape :', shape)
    if args.time_execution:
        print('    time : %.6f sec' % (time.time()-t0))

baryon_chemical_potential = baryon_chemical_potential.reshape(shape)
temperature = temperature.reshape(shape)
speed_of_sound = speed_of_sound.reshape(shape)

### throw away the boring part of the EoS

keep = temperature[:,0] <= args.max_temperature

baryon_chemical_potential = baryon_chemical_potential[keep,:]
temperature = temperature[keep,:]
speed_of_sound = speed_of_sound[keep,:]

if args.verbose:
    print('    downselected data to temperatures below %.3f MeV; retained shape : %s' % \
        (args.max_temperature, np.shape(speed_of_sound)))

### now decimate the data so that the covariance matrices we build fit in memory

baryon_chemical_potential = baryon_chemical_potential[::args.downsample, ::args.downsample]
temperature = temperature[::args.downsample, ::args.downsample]
speed_of_sound = speed_of_sound[::args.downsample, ::args.downsample]

if args.verbose:
    print('    further downselected data to shape :', np.shape(speed_of_sound))

#------------------------

# let's then divide this into training and testing sets
# we divid the grid in the following way: (o --> training set, x --> test set)
#     o o o o o
#     o x o x o
#     o o o o o
#     o x o x o
#     o o o o o

if args.verbose:
    print('formatting training and test sets')
    if args.time_execution:
        t0 = time.time()

source_x = []
source_f = []
target_x = []
target_f = []

for ind in range(len(speed_of_sound)):
    offset = ind % 2 ### figure out whether this is an odd or an even row

    if offset: # an odd row, so only take every other sample for training set
        # grab the training data
        source_x.append(np.transpose([baryon_chemical_potential[ind, ::2], temperature[ind, ::2]]))
        source_f.append(speed_of_sound[ind, ::2])

        # grab the test data
        target_x.append(np.transpose([baryon_chemical_potential[ind, 1::2], temperature[ind, 1::2]]))
        target_f.append(speed_of_sound[ind,1::2])

    else: # an even row, so grab everything
        # grab the training data
        source_x.append(np.transpose([baryon_chemical_potential[ind,:], temperature[ind,:]]))
        source_f.append(speed_of_sound[ind,:])

source_x = np.concatenate(tuple(source_x))
source_f = np.concatenate(tuple(source_f))

target_x = np.concatenate(tuple(target_x))
target_f = np.concatenate(tuple(target_f))

if args.verbose:
    print('    selected:\n        %d training points\n        %d test points'%(len(source_x), len(target_x)))
    if args.time_execution:
        print('    time : %.6f sec' % (time.time()-t0))

#------------------------

if args.verbose:
    print('estimating appropriate hyperparameters and constructing kernel')
    if args.time_execution:
        t0 = time.time()

# now, let's guess at appropriate hyperparameters
ave = np.mean(source_f)       ### mean of the process

### FIXME! pick these based on maximizing the marginal likelihood for (source_f, source_x)

sigma = np.std(source_f) * 1.5  ### variance based on spread of the function

lengths = [                   ### based on the average grid spacing
    np.mean(np.diff(baryon_chemical_potential[0,:])) * 2.0,
    np.mean(np.diff(temperature[:,0])) * 2.0,
]

# construct a kernel
kernel = mgpi.CombinedKernel(
    mgpi.SquaredExponentialKernel(sigma, *lengths),
    mgpi.WhiteNoiseKernel(sigma/1000), # a small white noise component for numerical stability
)

if args.verbose:
    print(kernel)

# construct a general interpolator
basic = mgpi.Interpolator(kernel)

nngp = mgpi.NearestNeighborInterpolator(
    kernel,
    num_neighbors=args.num_neighbors,
    order_by_index=args.order_by_index,
)

if args.time_execution:
    print('    time : %.6f sec' % (time.time()-t0))

# pre-sort the data
sorted_x, sorted_f = nngp._2sorted(source_x, source_f=source_f)
neighbors = nngp._2neighbors(sorted_x)

#-------------------------------------------------
#
# time likelihood evaluations
#
#-------------------------------------------------

if args.verbose:
    print('performing %d trials to estimate runtime of loglikelihood calls' % args.num_trials)

basic_times = []
nngp_times = []
psnn_times = []

for trial in range(args.num_trials):
    if args.verbose:
        sys.stdout.write('\r    trial %d / %d' % (trial, args.num_trials))
        sys.stdout.flush()

    # compute for basic interpolator
    t0 = time.time()
    basic.loglikelihood(source_x, source_f)
    basic_times.append(time.time()-t0)

    # compute for nngp interpolator
    t0 = time.time()
    nngp.loglikelihood(source_x, source_f)
    nngp_times.append(time.time()-t0)

    # compute for nngp interpolator with pre-sorted data
    t0 = time.time()
    nngp.loglikelihood(sorted_x, sorted_f, neighbors=neighbors)
    psnn_times.append(time.time()-t0)

if args.verbose:
    sys.stdout.write('\n')
    sys.stdout.flush()

print('Interpolator.loglikelihood\n    mean : %.3e sec\n    stdv : %.3e sec' % \
    (np.mean(basic_times), np.std(basic_times)))
print('NearestNeighborInterpolator.loglikelihood\n    mean : %.3e sec\n    stdv : %.3e sec' % \
    (np.mean(nngp_times), np.std(nngp_times)))
print('NearestNeighborInterpolator.loglikelihood with pre-sorting\n    mean : %.3e sec\n    stdv : %.3e sec' % \
    (np.mean(psnn_times), np.std(psnn_times)))
